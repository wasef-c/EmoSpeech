{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:36<00:00,  7.25s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Replace 'your_dataset_name' with the name of your Hugging Face dataset\n",
    "dataset = load_dataset(\"cairocode/IEMO_WAV_002\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the emotion mapping\n",
    "emotion_mapping = {\n",
    "    \"Neutral\": 0,\n",
    "    \"Happy\": 1,\n",
    "    \"Sad\": 2,\n",
    "    \"Angry\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Define the filter function with an argument\n",
    "def create_filter_function(spk_id):\n",
    "    def filter_m_examples(example):\n",
    "        return example[\"label\"] != 4 and example[\"label\"] != 5 and example[\"speakerID\"] == spk_id\n",
    "    return filter_m_examples\n",
    "\n",
    "# Use the filter function with the specific speaker ID\n",
    "spk_id = 2\n",
    "filter_function = create_filter_function(spk_id)\n",
    "ds_002 = dataset['train'].filter(filter_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'label', 'valence', 'arousal', 'domination', 'arousal_norm', 'valence_norm', 'speakerID', 'utterance_id', 'transcript', 'speaker_id'],\n",
      "    num_rows: 1340\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def group_by_label(dataset, label_key):\n",
    "    \"\"\"Group dataset samples by label.\"\"\"\n",
    "    label_groups = defaultdict(list)\n",
    "    for sample in dataset:\n",
    "        label = sample[label_key]\n",
    "        label_groups[label].append(sample)\n",
    "    return label_groups\n",
    "\n",
    "def create_balanced_batches(dataset, label_key, batch_size):\n",
    "    \"\"\"Create balanced batches ensuring at least one sample per label.\"\"\"\n",
    "    label_groups = group_by_label(dataset, label_key)\n",
    "    batches = []\n",
    "    labels = list(label_groups.keys())\n",
    "    \n",
    "    while any(len(label_groups[label]) > 0 for label in labels):\n",
    "        batch = []\n",
    "        random.shuffle(labels)  # Shuffle labels for randomness\n",
    "        for label in labels:\n",
    "            if len(batch) < batch_size and len(label_groups[label]) > 0:\n",
    "                batch.append(label_groups[label].pop(0))  # Take one sample\n",
    "        \n",
    "        # Fill the remaining spots in the batch\n",
    "        remaining_samples = [\n",
    "            sample for label in labels\n",
    "            for sample in label_groups[label][:max(0, batch_size - len(batch))]\n",
    "        ]\n",
    "        random.shuffle(remaining_samples)  # Add randomness to the selection\n",
    "        batch.extend(remaining_samples[:batch_size - len(batch)])\n",
    "        \n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def balanced_batches_as_datasets(dataset, label_key, batch_size):\n",
    "    \"\"\"Create balanced batches and return them as a list of Dataset objects.\"\"\"\n",
    "    batches = create_balanced_batches(dataset, label_key, batch_size)\n",
    "    dataset_batches = [\n",
    "        Dataset.from_dict({key: [sample[key] for sample in batch] for key in dataset.column_names})\n",
    "        for batch in batches\n",
    "    ]\n",
    "    return dataset_batches\n",
    "\n",
    "# Example usage\n",
    "# Assuming `train_dataset` is a Hugging Face Dataset and labels are in the 'label' column\n",
    "batch_size = 8\n",
    "balanced_dataset_batches = balanced_batches_as_datasets(ds, 'label', batch_size)\n",
    "\n",
    "# Combine batches back into a single Dataset if needed\n",
    "final_dataset = Dataset.from_dict({\n",
    "    key: [sample for batch in balanced_dataset_batches for sample in batch[key]]\n",
    "    for key in ds.column_names\n",
    "})\n",
    "\n",
    "print(final_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8 range(0, 8)\n",
      " | Actual Label: 1 | Predicted Emotion: 0 | Transcript: Well Vegas was awesome.\n",
      " | Actual Label: 3 | Predicted Emotion: 1 | Transcript: That's out of control.\n",
      " | Actual Label: 0 | Predicted Emotion: 3 | Transcript: Excuse me.\n",
      " | Actual Label: 2 | Predicted Emotion: 0 | Transcript: Did you get the mail? So you saw my letter?\n",
      " | Actual Label: 3 | Predicted Emotion: 2 | Transcript: The bus, I'm riding the bus because I want to.\n",
      " | Actual Label: 1 | Predicted Emotion: 3 | Transcript: It was at the slot machines.\n",
      " | Actual Label: 2 | Predicted Emotion: 1 | Transcript: I don't know.  I put in that. request too. They didn't...\n",
      " | Actual Label: 0 | Predicted Emotion: 2 | Transcript: Clearly.  You know, do you have like a supervisor or something?\n",
      "8 16 range(8, 16)\n",
      " | Actual Label: 2 | Predicted Emotion: 0 | Transcript: Yeah.  I know.\n",
      " | Actual Label: 1 | Predicted Emotion: 2 | Transcript: And, um, I got married.\n",
      " | Actual Label: 3 | Predicted Emotion: 1 | Transcript: I'm not frustrated.\n",
      " | Actual Label: 0 | Predicted Emotion: 3 | Transcript: Yeah.\n",
      " | Actual Label: 3 | Predicted Emotion: 0 | Transcript: I can not-- you are not here by choice.  Nobody would ride this bus by choice.\n",
      " | Actual Label: 3 | Predicted Emotion: 3 | Transcript: The bus, I'm riding the bus because I want to.\n",
      " | Actual Label: 2 | Predicted Emotion: 3 | Transcript: There's people that have given more though, you know?\n",
      " | Actual Label: 2 | Predicted Emotion: 2 | Transcript: things just aren't what they seem.\n",
      "16 24 range(16, 24)\n",
      " | Actual Label: 3 | Predicted Emotion: 0 | Transcript: You are so high and mighty.  I just--\n",
      " | Actual Label: 2 | Predicted Emotion: 3 | Transcript: I don't know.  I put in that. request too. They didn't...\n",
      " | Actual Label: 1 | Predicted Emotion: 2 | Transcript: Yeah.  In the old town part.\n",
      " | Actual Label: 0 | Predicted Emotion: 1 | Transcript: Is there a problem?\n",
      " | Actual Label: 3 | Predicted Emotion: 0 | Transcript: The bus, I'm riding the bus because I want to.\n",
      " | Actual Label: 1 | Predicted Emotion: 3 | Transcript: Yeah.  Well.  You know, because he's leaving the next day.\n",
      " | Actual Label: 1 | Predicted Emotion: 1 | Transcript: Uhuh, uhuh.  He won big and he-and he realized that the only thing that would make it better was me as his bride.\n",
      " | Actual Label: 2 | Predicted Emotion: 1 | Transcript: things just aren't what they seem.\n",
      "24 32 range(24, 32)\n",
      " | Actual Label: 2 | Predicted Emotion: 0 | Transcript: There's people that have given more though, you know?\n",
      " | Actual Label: 1 | Predicted Emotion: 2 | Transcript: Um- Yes.  It was very romantic.\n",
      " | Actual Label: 0 | Predicted Emotion: 1 | Transcript: Well what's the problem?  Let me change it.\n",
      " | Actual Label: 3 | Predicted Emotion: 0 | Transcript: Oh.  I'm saving the environment. I'm riding this car because I- I want to.\n",
      " | Actual Label: 3 | Predicted Emotion: 3 | Transcript: I can not-- you are not here by choice.  Nobody would ride this bus by choice.\n",
      " | Actual Label: 3 | Predicted Emotion: 3 | Transcript: Flight seven fourteen.\n",
      " | Actual Label: 0 | Predicted Emotion: 3 | Transcript: I have to find sitters.\n",
      " | Actual Label: 2 | Predicted Emotion: 0 | Transcript: If not me then, who?\n",
      "32 40 range(32, 40)\n",
      " | Actual Label: 3 | Predicted Emotion: 0 | Transcript: The bus, I'm riding the bus because I want to.\n",
      " | Actual Label: 0 | Predicted Emotion: 3 | Transcript: Clearly.  You know, do you have like a supervisor or something?\n",
      " | Actual Label: 2 | Predicted Emotion: 0 | Transcript: Just, you know, kicking myself.\n",
      " | Actual Label: 1 | Predicted Emotion: 2 | Transcript: It was at the slot machines.\n",
      " | Actual Label: 3 | Predicted Emotion: 1 | Transcript: Is this a joke?\n",
      " | Actual Label: 1 | Predicted Emotion: 3 | Transcript: Yeah.  Well.  You know, because he's leaving the next day.\n",
      " | Actual Label: 0 | Predicted Emotion: 1 | Transcript: I guess, you know, everybody has to make sacrifices.\n",
      " | Actual Label: 1 | Predicted Emotion: 0 | Transcript: Uhuh, uhuh.  He won big and he-and he realized that the only thing that would make it better was me as his bride.\n",
      "40 48 range(40, 48)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m speaker_ids \u001b[38;5;241m=\u001b[39m [ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeakerID\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Process conversations\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m conversations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     46\u001b[0m     [\n\u001b[0;32m     47\u001b[0m         {\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     50\u001b[0m                 \u001b[38;5;66;03m# Only add \"The correct answer was ...\" if it's not the first in the batch\u001b[39;00m\n\u001b[0;32m     51\u001b[0m                 \u001b[38;5;241m*\u001b[39m(\n\u001b[0;32m     52\u001b[0m                     [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe correct answer was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreverse_emotion_mapping[actual_labels[i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mbatch_start]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use this to understand this person more.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     53\u001b[0m                     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m batch_start \u001b[38;5;129;01mand\u001b[39;00m i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     54\u001b[0m                     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m     55\u001b[0m                 ),\n\u001b[0;32m     56\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m: ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m][i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[0;32m     57\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs she happy, sad, angry or neutral? Answer in one word\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     58\u001b[0m             ]\n\u001b[0;32m     59\u001b[0m         }\n\u001b[0;32m     60\u001b[0m     ]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices\n\u001b[0;32m     62\u001b[0m ]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(i)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Prepare text inputs\u001b[39;00m\n\u001b[0;32m     68\u001b[0m text \u001b[38;5;241m=\u001b[39m [processor\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m conversations]\n",
      "Cell \u001b[1;32mIn[33], line 56\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m speaker_ids \u001b[38;5;241m=\u001b[39m [ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeakerID\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Process conversations\u001b[39;00m\n\u001b[0;32m     45\u001b[0m conversations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     46\u001b[0m     [\n\u001b[0;32m     47\u001b[0m         {\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     50\u001b[0m                 \u001b[38;5;66;03m# Only add \"The correct answer was ...\" if it's not the first in the batch\u001b[39;00m\n\u001b[0;32m     51\u001b[0m                 \u001b[38;5;241m*\u001b[39m(\n\u001b[0;32m     52\u001b[0m                     [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe correct answer was \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreverse_emotion_mapping[actual_labels[i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mbatch_start]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use this to understand this person more.\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[0;32m     53\u001b[0m                     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m batch_start \u001b[38;5;129;01mand\u001b[39;00m i\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     54\u001b[0m                     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m     55\u001b[0m                 ),\n\u001b[1;32m---> 56\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m]},\n\u001b[0;32m     57\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs she happy, sad, angry or neutral? Answer in one word\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     58\u001b[0m             ]\n\u001b[0;32m     59\u001b[0m         }\n\u001b[0;32m     60\u001b[0m     ]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_indices\n\u001b[0;32m     62\u001b[0m ]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(i)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Prepare text inputs\u001b[39;00m\n\u001b[0;32m     68\u001b[0m text \u001b[38;5;241m=\u001b[39m [processor\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m conversation \u001b[38;5;129;01min\u001b[39;00m conversations]\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2747\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2746\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2747\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:405\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:148\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\pyarrow\\table.pxi:1366\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\pyarrow\\array.pxi:1656\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\Documents\\carol_emo_rec\\MLLM\\.venv\\lib\\site-packages\\pyarrow\\scalar.pxi:794\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\_collections_abc.py:775\u001b[0m, in \u001b[0;36mMapping.keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD.keys() -> a set-like object providing a view on D\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms keys\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m KeysView(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "import librosa\n",
    "import warnings\n",
    "import torch\n",
    "import csv\n",
    "warnings.filterwarnings(\"ignore\", message=\"It is strongly recommended to pass the `sampling_rate` argument\")\n",
    "\n",
    "# Map emotions to numbers\n",
    "emotion_mapping = {\n",
    "    \"Neutral\": 0,\n",
    "    \"Happy\": 1,\n",
    "    \"Sad\": 2,\n",
    "    \"Angry\": 3\n",
    "}\n",
    "# Reverse mapping: from number to emotion\n",
    "reverse_emotion_mapping = {v: k for k, v in emotion_mapping.items()}\n",
    "ds = final_dataset\n",
    "# Load processor and model\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "# model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "csv_file = \"emotion_results_all.csv\"\n",
    "fieldnames = [\"Transcript\", \"Label\", \"Predicted Emotion\", \"Mapped Value\", \"speaker_id\"]\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "batch_size = 8  # Adjust batch size as needed\n",
    "num_samples = len(ds)  # Replace `ds` with your dataset\n",
    "\n",
    "for batch_start in range(0, num_samples, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, num_samples)\n",
    "    batch_indices = range(batch_start, batch_end)\n",
    "    print(batch_start, batch_end, batch_indices)\n",
    "\n",
    "\n",
    "    transcripts = [ds['transcript'][i] for i in batch_indices]\n",
    "    actual_labels = [ds['label'][i] for i in batch_indices]\n",
    "    speaker_ids = [ds['speakerID'][i] for i in batch_indices]\n",
    "\n",
    "        # Process conversations\n",
    "    conversations = [\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    # Only add \"The correct answer was ...\" if it's not the first in the batch\n",
    "                    *(\n",
    "                        [{\"type\": \"text\", \"text\": f\"The correct answer was {reverse_emotion_mapping[actual_labels[i - 1 - batch_start]]}, use this to understand this person more.\"}]\n",
    "                        if i > batch_start and i>0\n",
    "                        else []\n",
    "                    ),\n",
    "                    {\"type\": \"audio\", \"array\": ds['audio'][i]['array']},\n",
    "                    {\"type\": \"text\", \"text\": \"Is she happy, sad, angry or neutral? Answer in one word\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        for i in batch_indices\n",
    "    ]\n",
    "    # print(i)\n",
    "\n",
    "\n",
    "\n",
    "    # Prepare text inputs\n",
    "    text = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\n",
    "\n",
    "    # Prepare audio inputs\n",
    "    audios = []\n",
    "    for conversation in conversations:\n",
    "        for message in conversation:\n",
    "            if isinstance(message[\"content\"], list):\n",
    "                for ele in message[\"content\"]:\n",
    "                    if ele[\"type\"] == \"audio\":\n",
    "                        # Convert numpy array to WAV and load with librosa\n",
    "                        audio_data = ele[\"array\"]\n",
    "                        with BytesIO() as buffer:\n",
    "                            sf.write(buffer, audio_data, samplerate=16000, format=\"WAV\")  # Assuming 16kHz input\n",
    "                            buffer.seek(0)\n",
    "                            y, _ = librosa.load(buffer, sr=processor.feature_extractor.sampling_rate)\n",
    "                            audios.append(y.astype(np.float32))\n",
    "\n",
    "    # Ensure the number of text and audio samples match\n",
    "    if len(audios) != len(text):\n",
    "        raise ValueError(f\"Mismatch between audio ({len(audios)}) and text ({len(text)}) inputs.\")\n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audios=audios,\n",
    "        sampling_rate=16000,  # Explicitly set the sampling rate here\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Adjust attention_mask for valid start tokens\n",
    "    inputs[\"attention_mask\"][:, 0] = 1\n",
    "\n",
    "    # Handle special audio token alignment\n",
    "    special_audio_token_mask = (inputs[\"input_ids\"] == model.config.audio_token_index)\n",
    "\n",
    "    # Ensure special_audio_token_mask is valid for audio processing\n",
    "    if special_audio_token_mask.sum() == 0:\n",
    "        audio_token_id = getattr(model.config, \"audio_token_index\", None)\n",
    "        if audio_token_id is not None:\n",
    "            inputs[\"input_ids\"][:, 0] = audio_token_id\n",
    "\n",
    "    # Ensure batch size matches across all tensors\n",
    "    text_batch_size = inputs[\"input_ids\"].shape[0]\n",
    "    audio_batch_size = inputs[\"input_features\"].shape[0]\n",
    "\n",
    "    if text_batch_size != audio_batch_size:\n",
    "        raise RuntimeError(\n",
    "            f\"Batch size mismatch: Text batch size ({text_batch_size}) \"\n",
    "            f\"does not match Audio batch size ({audio_batch_size}).\"\n",
    "        )\n",
    "\n",
    "    # Move tensors to the appropriate device\n",
    "    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n",
    "\n",
    "    # Generate response\n",
    "    try:\n",
    "        generate_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            input_features=inputs[\"input_features\"],\n",
    "            feature_attention_mask=inputs[\"feature_attention_mask\"],\n",
    "            max_length=256\n",
    "        )\n",
    "\n",
    "        # Post-process generation output\n",
    "        generate_ids = generate_ids[:, inputs[\"input_ids\"].size(1):]\n",
    "\n",
    "        # Decode the responses\n",
    "        responses = processor.batch_decode(\n",
    "            generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        for i, response in enumerate(responses):\n",
    "            predicted_emotion = response.strip().capitalize()\n",
    "            mapped_value = emotion_mapping.get(predicted_emotion, -1)\n",
    "\n",
    "            print(f\" | Actual Label: {actual_labels[i]} | Predicted Emotion: {mapped_value} | Transcript: {transcripts[i]}\")\n",
    "\n",
    "            # Save results to CSV\n",
    "            with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "                writer.writerow({\n",
    "                    \"Transcript\": transcripts[i],\n",
    "                    \"Label\": actual_labels[i],\n",
    "                    \"Predicted Emotion\": predicted_emotion,\n",
    "                    \"Mapped Value\": mapped_value,\n",
    "                    \"speaker_id\": speaker_ids[i]\n",
    "                })\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(\"RuntimeError during generation:\", str(e))\n",
    "        print(\"Debugging info:\")\n",
    "        print(f\"Input IDs Shape: {inputs['input_ids'].shape}\")\n",
    "        print(f\"Feature Attention Mask Shape: {inputs['feature_attention_mask'].shape}\")\n",
    "        print(f\"Input Features Shape: {inputs['input_features'].shape}\")\n",
    "        print(f\"Attention Mask Shape: {inputs['attention_mask'].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
