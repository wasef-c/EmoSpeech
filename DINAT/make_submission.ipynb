{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating test split: 100%|██████████| 26248/26248 [00:01<00:00, 23345.67 examples/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nDinatForImageClassification requires the natten library but it was not found in your environment. You can install it by referring to:\nshi-labs.com/natten . You can also install it with pip (may take longer to build):\n`pip install natten`. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     66\u001b[0m test_dataset\u001b[38;5;241m.\u001b[39mset_transform(test_transforms)\n\u001b[0;32m     68\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     69\u001b[0m     test_dataset,\n\u001b[0;32m     70\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     71\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn_test,\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m image_model \u001b[38;5;241m=\u001b[39m \u001b[43mDinatForImageClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     83\u001b[0m image_processor \u001b[38;5;241m=\u001b[39m DinatForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(image_model_name)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     86\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name)\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\transformers\\modeling_utils.py:4130\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4124\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   4125\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   4126\u001b[0m     )\n\u001b[0;32m   4128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   4129\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 4130\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4132\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   4133\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\transformers\\models\\dinat\\modeling_dinat.py:772\u001b[0m, in \u001b[0;36mDinatForImageClassification.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m--> 772\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnatten\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_labels\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinat \u001b[38;5;241m=\u001b[39m DinatModel(config)\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nDinatForImageClassification requires the natten library but it was not found in your environment. You can install it by referring to:\nshi-labs.com/natten . You can also install it with pip (may take longer to build):\n`pip install natten`. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from functions_old import *\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    DinatForImageClassification,\n",
    "    BertModel,\n",
    "    AutoTokenizer,\n",
    "    get_scheduler,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def collate_fn_test(examples):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batching of image data and BERT inputs.\n",
    "    \"\"\"\n",
    "    # print(examples)\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"]\n",
    "                               for example in examples]).to(device)\n",
    "    input_ids = torch.stack([example[\"input_ids\"]\n",
    "                            for example in examples]).to(device)\n",
    "    attention_mask = torch.stack(\n",
    "        [example[\"attention_mask\"] for example in examples]).to(device)\n",
    "    files = [example[\"file\"] for example in examples]\n",
    "\n",
    "    bert_embeddings = torch.stack(\n",
    "        [example[\"bert_embeddings\"] for example in examples]).to(device)\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"files\": files,\n",
    "        \"bert_embeddings\": bert_embeddings\n",
    "    }\n",
    "\n",
    "checkpoint_path = r\"D:\\Documents\\MASC\\NLP_EMO\\MODELS\\best_model_VAL.pt\"\n",
    "output_dir = \"./results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Pre-trained model names (ViT + BERT)\n",
    "image_model_name = \"shi-labs/dinat-mini-in1k-224\"\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "dataset_name = 'cairocode/MSPP_TEST_SYL'\n",
    "# test_dataset = \n",
    "test_dataset = load_dataset(dataset_name)['test']\n",
    "\n",
    "test_dataset.set_transform(test_transforms)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=50,\n",
    "    collate_fn=collate_fn_test,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "image_model = DinatForImageClassification.from_pretrained(\n",
    "    image_model_name,\n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    problem_type=\"regression\",\n",
    ").to(device)\n",
    "\n",
    "image_processor = DinatForImageClassification.from_pretrained(image_model_name).to(device)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "base_bert_model = BertModel.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Initialize the Combined Regression Model\n",
    "# ----------------------------------------------------------------------\n",
    "model = CombinedModelsBi(\n",
    "    image_model=image_model,\n",
    "    bert_model=bert_model,\n",
    "    image_feature_dim=512,\n",
    "    bert_embedding_dim=768,\n",
    "    combined_dim=512,\n",
    "    num_labels=1,\n",
    "\n",
    ").to(device)\n",
    "\n",
    "\n",
    "if checkpoint_path != None:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Evaluation\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\nStarting Test Evaluation...\")\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "all_test_predictions = []\n",
    "all_test_labels = []\n",
    "all_filenames = []  # To store filenames\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "    for batch in test_progress_bar:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        # labels = batch[\"labels\"].float().to(device)\n",
    "\n",
    "        # Extract filenames from the batch (assuming \"filename\" is part of batch)\n",
    "        filenames = batch[\"files\"]  # Adjust key as necessary\n",
    "        all_filenames.extend(filenames)\n",
    "\n",
    "        outputs_dict = model(\n",
    "            pixel_values=pixel_values,\n",
    "            bert_input_ids=input_ids,\n",
    "            bert_attention_mask=attention_mask\n",
    "        )\n",
    "        predictions = outputs_dict[\"logits\"].squeeze(-1)\n",
    "\n",
    "        # loss = criterion(predictions, labels)\n",
    "        # test_loss += loss.item()\n",
    "\n",
    "        all_test_predictions.extend(predictions.cpu().numpy())\n",
    "        # all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "output_csv_path = os.path.join(output_dir, \"valence.csv\")\n",
    "with open(output_csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Filename\", \"Valence\"])\n",
    "    for filename, prediction in zip(all_filenames, all_test_predictions):\n",
    "        writer.writerow([filename, prediction])\n",
    "\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "# print(dataset_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
