{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "1. CMU MOSEI\n",
      "2. CREMA-D\n",
      "3. EMOV-DB\n",
      "4. MSP IMPROV\n",
      "5. RAVDESS-DB\n",
      "6. TESS-DB\n",
      "7. VIVAE-DB\n",
      "8. IEMOCAP\n",
      "9. ASVP-ESD\n",
      "10. OMG\n",
      "11. MSP_POD\n",
      "12. MSP_POD_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import parselmouth\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Define constants\n",
    "DATASETS_PATH = r'D:\\Documents\\MASC\\MSP_POD_dataset'\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.io import wavfile\n",
    "from PIL import Image\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "def clean_audio(path):\n",
    "    y, sr = librosa.load(path)\n",
    "    S_full, phase = librosa.magphase(librosa.stft(y))\n",
    "    idx = slice(*librosa.time_to_frames([2, 6], sr=sr))\n",
    "    width = int((S_full.shape[-1] - 1)/2)-1\n",
    "    S_filter = librosa.decompose.nn_filter(S_full,\n",
    "                                           aggregate=np.median,\n",
    "                                           metric='cosine',\n",
    "                                           width=width)\n",
    "    S_filter = np.minimum(S_full, S_filter)\n",
    "    margin_i, margin_v = 2, 10\n",
    "    power = 2\n",
    "\n",
    "    mask_i = librosa.util.softmask(S_filter,\n",
    "                                   margin_i * (S_full - S_filter),\n",
    "                                   power=power)\n",
    "\n",
    "    mask_v = librosa.util.softmask(S_full - S_filter,\n",
    "                                   margin_v * S_filter,\n",
    "                                   power=power)\n",
    "\n",
    "    S_foreground = mask_v * S_full\n",
    "\n",
    "    sound = librosa.istft(S_foreground * phase)\n",
    "    # sound = y\n",
    "    # # # sf.write(os.path.join(new_dir,new_name), librosa.istft(S_foreground * phase), sr)\n",
    "    return sound, sr\n",
    "# Step 1: Create a Mel-Spectrogram\n",
    "def create_mel_spectrogram(audio, sr):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    return mel_spec\n",
    "\n",
    "# Step 2: Extract syllables (this is a simplified approach)\n",
    "def extract_syllables(mel_spec):\n",
    "    energy = np.sum(mel_spec, axis=0)\n",
    "    peaks, _ = find_peaks(energy, distance=20)  # Adjust distance as needed\n",
    "    if len(peaks) < 2:\n",
    "        # If less than 2 peaks found, use start and end of the spectrogram\n",
    "        peaks = np.array([0, mel_spec.shape[1] - 1])\n",
    "    return peaks\n",
    "\n",
    "# Step 3: Extract formants for a specific time range\n",
    "def extract_formants_for_syllable(audio, sr, start_time, end_time):\n",
    "    sound = parselmouth.Sound(audio, sampling_frequency=sr)\n",
    "    syllable_sound = sound.extract_part(from_time=start_time, to_time=end_time)\n",
    "    formant = syllable_sound.to_formant_burg()\n",
    "    return formant\n",
    "\n",
    "# Step 4: Plot formants and spectrogram for each syllable\n",
    "def plot_syllable(fname, image_dir,mel_spec, sr, syl_val, syllable_start, syllable_end, formant):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(2.24, 2.24))\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spec[:, syllable_start:syllable_end], ref=np.max), ax=ax1)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # Plot formants\n",
    "    times = formant.xs()\n",
    "    formant_values = []\n",
    "    for t in times:\n",
    "        for i in range(1, 4):\n",
    "            value = formant.get_value_at_time(formant_number=i, time=t)\n",
    "            if value is not None:\n",
    "                formant_values.append(value)\n",
    "    \n",
    "    # Ensure times and formant_values have the same length\n",
    "    times_repeated = np.repeat(times, 3)[:len(formant_values)]\n",
    "    \n",
    "    ax2.scatter(times_repeated, formant_values, s=1)\n",
    "    ax2.set_ylim(0, 5000)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    jpname = f\"{fname}_syl{syl_val}.png\"\n",
    "    save_path = os.path.join(image_dir, jpname)\n",
    "\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    return jpname\n",
    "\n",
    "# Main function to process the audio\n",
    "def process_audio(audio_path, fname, image_dir):\n",
    "    jpnames = []\n",
    "    # Clean the audio\n",
    "    audio, sr = clean_audio(audio_path)\n",
    "    \n",
    "    # Create mel spectrogram\n",
    "    mel_spec = create_mel_spectrogram(audio, sr)\n",
    "    \n",
    "    # Extract syllables\n",
    "    syllable_boundaries = extract_syllables(mel_spec)\n",
    "    \n",
    "    # Process each syllable\n",
    "\n",
    "    for i in range(len(syllable_boundaries) - 1):\n",
    "        start_frame = syllable_boundaries[i]\n",
    "        end_frame = syllable_boundaries[i+1]\n",
    "        \n",
    "        # Convert frames to time\n",
    "        start_time = librosa.frames_to_time(start_frame, sr=sr)\n",
    "        end_time = librosa.frames_to_time(end_frame, sr=sr)\n",
    "        \n",
    "        # Extract formants for the syllable\n",
    "        formant = extract_formants_for_syllable(audio, sr, start_time, end_time)\n",
    "        \n",
    "        # Plot the syllable\n",
    "        jpname= plot_syllable(fname,image_dir, mel_spec, sr, i,start_frame, end_frame, formant)\n",
    "        jpnames.append(jpname)\n",
    "    return  jpnames \n",
    "\n",
    "\n",
    "def process_files(image_dir, csv_dir, ds=0):\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    csv_file = os.path.join(csv_dir, 'file_labels.csv')\n",
    "\n",
    "    existing_files = set()\n",
    "    if os.path.exists(csv_file):\n",
    "        with open(csv_file, 'r', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                existing_files.add(row['file_name'])\n",
    "\n",
    "\n",
    "\n",
    "    for filename in os.listdir(csv_dir):\n",
    "        if filename.endswith('.wav'):  # Check for .wav extension\n",
    "            file_path = os.path.join(csv_dir, filename)\n",
    "            if os.path.isfile(file_path):  # Ensure it's a file\n",
    "                fname = os.path.basename(file_path)\n",
    "                jpnames =  process_audio(file_path, fname, image_dir)\n",
    "\n",
    "\n",
    "    # with open(csv_file, 'a', newline='') as csvfile:\n",
    "    #     fieldnames = ['file_name', 'full_path', 'speaker', 'full_path','arousal', 'valence', '']\n",
    "    #     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    #     if os.path.getsize(csv_file) == 0:\n",
    "    #         writer.writeheader()\n",
    "\n",
    "    #     total_new_images = 0\n",
    "    #     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing audio files\"):\n",
    "    #         file_path = row['filename']\n",
    "    #         file_name = row['fname']\n",
    "    #         label_val = row['label']\n",
    "    #         speaker = row['speaker']\n",
    "    #         arousal = row['act']\n",
    "    #         valence = row['val']\n",
    "    #         domination = row['']\n",
    "            \n",
    "    #         if ds == 0:\n",
    "    #             fname = str(file_name.split('.wav')[0])\n",
    "    #         else:\n",
    "    #             indval = file_path.rfind('/', 0, file_path.rfind('/'))\n",
    "    #             vid_utter = file_path[indval+1:]\n",
    "    #             fname = vid_utter.replace(\"/\", \"_\")\n",
    "            \n",
    "    #         jpnames =  process_audio(file_path, fname, image_dir)\n",
    "\n",
    "    #         for jpname in jpnames:\n",
    "    #             full__path = os.path.join(image_dir, jpname)\n",
    "    #             writer.writerow({\n",
    "    #                 'file_name': jpname,\n",
    "    #                 'label': label_val,\n",
    "    #                 'speaker': speaker,\n",
    "    #                 'full_path': full__path,\n",
    "    #                 'arousal': arousal,\n",
    "    #                 'valence': valence,\n",
    "    #                 'domination' : domination,\n",
    "    #             })\n",
    "    \n",
    "\n",
    "def main():\n",
    "    datasets = {\n",
    "        1: (\"CMU MOSEI\", '/home/carol/Documents/Emo_rec/CSV_FILES/CMUmini_data.csv', '/media/carol/Data/DATASETS/SavedSets002/CMUmini'),\n",
    "        2: (\"CREMA-D\", '/home/carol/Documents/Emo_rec/CSV_FILES/CREMA_data.csv', '/media/carol/Data/DATASETS/SavedSets002/CREMA'),\n",
    "        3: (\"EMOV-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/EMOV_data.csv', '/media/carol/Data/DATASETS/SavedSets002/EMOV'),\n",
    "        4: (\"MSP IMPROV\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/MSPIMPROV_data2.csv', '/media/carol/Data/DATASETS/SavedSets002/MSPI_SYL'),\n",
    "        5: (\"RAVDESS-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/RAVDESS_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/RAVDESS'),\n",
    "        6: (\"TESS-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/TESS_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/TESS'),\n",
    "        7: (\"VIVAE-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/VIVAE_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/VIVAE'),\n",
    "        8: (\"IEMOCAP\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/IEMOCAP_data_Full.csv', '/media/carol/Data/DATASETS/SavedSets002/IEMOCAP_SYL'),\n",
    "        9: (\"ASVP-ESD\", '/home/carol/Documents/Emo_rec/CSV_FILES/ASVP_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/ASVP'),\n",
    "        10: (\"OMG\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/OMG_Train.csv', '/media/carol/Data/DATASETS/SavedSets002/OMG_Train2'),\n",
    "        11: (\"MSP_POD\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/MSPpod_data.csv', '/media/carol/Data/DATASETS/SavedSets002/MSP_POD_MEL'),\n",
    "        12:(\"MSP_POD_2\", r'D:\\Documents\\MASC\\MSP_POD_dataset\\Audios\\Audios.tar\\test', r'D:\\Documents\\MASC\\MSP_POD_dataset\\Images_Syllables_TEST')\n",
    "    }\n",
    "\n",
    "    print(\"Available datasets:\")\n",
    "    for key, (name, _, _) in datasets.items():\n",
    "        print(f\"{key}. {name}\")\n",
    "\n",
    "    dataset_choice = int(input(\"Enter the number of the dataset you would like to run: \"))\n",
    "\n",
    "    if dataset_choice in datasets:\n",
    "        name, file_path, output_dir = datasets[dataset_choice]\n",
    "        # print(f\"--------------{name} DATASET STARTED ---- \")\n",
    "\n",
    "        # df = pd.read_csv(file_path)\n",
    "        # image_dir = os.path.join(output_dir, 'images')\n",
    "        # os.makedirs(image_dir, exist_ok=True)\n",
    "        # os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        process_files(output_dir, file_path, dataset_choice)\n",
    "    else:\n",
    "        print(\"Invalid dataset choice.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created successfully and saved to D:\\Documents\\MASC\\MSP_POD_dataset\\Images_Syllables_TEST\\metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "image_folder = r\"D:\\Documents\\MASC\\MSP_POD_dataset\\Images_Syllables_TEST\"  # Replace with the path to your folder of .png files\n",
    "existing_csv_path = r\"D:\\Documents\\MASC\\MSP_POD_dataset\\Audios\\Audios.tar\\test\\metadata.csv\"  # Replace with the path to your existing CSV\n",
    "output_csv_path = os.path.join(image_folder, \"metadata.csv\") # Replace with the path to save the final CSV\n",
    "\n",
    "# Load the existing CSV\n",
    "existing_csv = pd.read_csv(existing_csv_path)\n",
    "\n",
    "# Create a mapping from wav_filename to transcript\n",
    "wav_to_transcript = dict(zip(existing_csv['file_name'], existing_csv['transcript']))\n",
    "\n",
    "# List all .png files in the folder\n",
    "png_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "# Prepare the final data\n",
    "data = []\n",
    "for png_file in png_files:\n",
    "    # Extract the .wav file name from the .png file name\n",
    "    wav_filename = png_file.split('.')[0] + '.wav'\n",
    "    extracted_number = png_file.split('_')[-1].split('.')[0]\n",
    "    # Get the mapped transcript\n",
    "    transcript = wav_to_transcript.get(wav_filename, \"Transcript not found\")\n",
    "    \n",
    "    # Append to the data list\n",
    "    data.append({\n",
    "        \"file_name\": png_file,\n",
    "        \"file\": wav_filename,\n",
    "        \"syllable\": extracted_number,\n",
    "        \"Transcript\": transcript\n",
    "    })\n",
    "\n",
    "# Create a DataFrame and save it as CSV\n",
    "output_df = pd.DataFrame(data)\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"CSV created successfully and saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 26249/26249 [00:01<00:00, 14893.98files/s]\n",
      "Generating test split: 26248 examples [00:02, 9892.92 examples/s] \n",
      "Map: 100%|██████████| 26248/26248 [01:56<00:00, 225.85 examples/s]]\n",
      "Creating parquet from Arrow format: 100%|██████████| 263/263 [00:00<00:00, 310.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [03:53<00:00, 233.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cairocode/MSPP_TEST_SYL/commit/0d038efa9bf2ae8fd96b2b2db5af342ef2e0f8dd', commit_message='Upload dataset', commit_description='', oid='0d038efa9bf2ae8fd96b2b2db5af342ef2e0f8dd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/cairocode/MSPP_TEST_SYL', endpoint='https://huggingface.co', repo_type='dataset', repo_id='cairocode/MSPP_TEST_SYL'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the image folder\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=image_folder)\n",
    "dataset.push_to_hub(\"MSPP_TEST_SYL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 333683/333683 [00:03<00:00, 88749.44 examples/s] \n",
      "Generating validation split: 100%|██████████| 83421/83421 [00:00<00:00, 90657.58 examples/s]\n",
      "Generating test split: 100%|██████████| 83421/83421 [00:00<00:00, 93817.50 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"cairocode/MSP_Pod_SYL6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets([ds['train'], ds['test'], ds['validation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500525/500525 [00:37<00:00, 13463.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_wav_and_syl(example):\n",
    "    # Extract wav_filename using regex\n",
    "    wav_match = re.search(r'(?<=Audios_)(.*?\\.wav)', example['full_path'])\n",
    "    syl_match = re.search(r'_(\\d+)\\.wav', example['full_path'])\n",
    "    \n",
    "    # Assign extracted values or None if no match\n",
    "    example['wav_filename'] = wav_match.group(1) if wav_match else None\n",
    "    example['syl_number'] = syl_match.group(1) if syl_match else None\n",
    "    return example\n",
    "\n",
    "# Map the function to the dataset\n",
    "updated_dataset = combined_dataset.map(extract_wav_and_syl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=224x223>,\n",
       " 'label': 1,\n",
       " 'speaker': 1,\n",
       " 'full_path': 'C:\\\\Users\\\\Paolo\\\\Documents\\\\carol_emo_rec\\\\DATASETS\\\\Image_Sets\\\\MSP_POD_SYL\\\\images\\\\Audios_MSP-PODCAST_0003_0360.wav_syl1.png',\n",
       " 'arousal': 6.076923,\n",
       " 'valence': 5.846154,\n",
       " 'wav_filename': 'MSP-PODCAST_0003_0360.wav',\n",
       " 'syl_number': '0360'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   file_name EmoClass  EmoAct  EmoVal  EmoDom  SpkrID  Gender  \\\n",
      "0  MSP-PODCAST_0002_0033.wav        N     4.8     4.2     5.4     127  Female   \n",
      "1  MSP-PODCAST_0002_0039.wav        N     4.0     4.2     4.2     127  Female   \n",
      "2  MSP-PODCAST_0002_0051.wav        N     4.0     4.2     4.2     127  Female   \n",
      "3  MSP-PODCAST_0002_0059.wav        X     4.0     3.8     4.0     128  Female   \n",
      "4  MSP-PODCAST_0002_0061.wav        F     3.4     2.8     4.2     128  Female   \n",
      "\n",
      "     Split_Set                                         transcript  \n",
      "0  Development               and i mean the numbers. right? so...  \n",
      "1  Development  15 hundred or something. it talks about the tw...  \n",
      "2  Development  [inaudible 00:03:13] so here it is. so it's pa...  \n",
      "3  Development  no. it sounds like it could be like a bourne i...  \n",
      "4  Development  yeah. so, but molly, i mean, this was really o...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_path = r\"D:\\Documents\\MASC\\MSP_POD_dataset\\Audios\\Audios.tar\\Audios\\metadata.csv\"\n",
    "\n",
    "# Load CSV into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500525/500525 [43:57<00:00, 189.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label', 'speaker', 'full_path', 'arousal', 'valence', 'wav_filename', 'syl_number'],\n",
      "    num_rows: 500525\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def add_csv_info(example):\n",
    "    matching_row = df[df['file_name'] == example['wav_filename']]\n",
    "    if not matching_row.empty:\n",
    "        for col in df.columns:\n",
    "            if col != 'file_name':  # Skip the file_name column\n",
    "                value = matching_row.iloc[0][col]\n",
    "                if pd.isnull(value):  # Handle missing values\n",
    "                    example[col] = None\n",
    "                elif isinstance(value, (int, float)):  # Keep numbers as is\n",
    "                    example[col] = value\n",
    "                else:  # Convert everything else to string\n",
    "                    example[col] = str(value)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            if col != 'file_name':\n",
    "                example[col] = None  # Default value for no match\n",
    "    return example\n",
    "\n",
    "\n",
    "# Map the function to the dataset\n",
    "updated_dataset2 = updated_dataset.map(add_csv_info)\n",
    "\n",
    "# Inspect the updated dataset\n",
    "print(updated_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500525/500525 [07:20<00:00, 1137.11 examples/s]\n",
      "Filter: 100%|██████████| 500525/500525 [05:11<00:00, 1605.97 examples/s]\n",
      "Filter: 100%|██████████| 500525/500525 [05:07<00:00, 1628.19 examples/s]\n",
      "Filter: 100%|██████████| 500525/500525 [05:11<00:00, 1607.61 examples/s]\n",
      "Map: 100%|██████████| 299922/299922 [29:55<00:00, 167.03 examples/s]\n",
      "Map:   1%|          | 999/100149 [00:08<14:41, 112.54 examples/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\PIL\\ImageFile.py:536\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    537\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 34\u001b[0m\n\u001b[0;32m     27\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m example: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m example: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m example: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m })\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Remove the 'split' column if no longer needed\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m dataset_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# # Verify the splits\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# print(dataset_dict)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\dataset_dict.py:887\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    886\u001b[0m     {\n\u001b[1;32m--> 887\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    907\u001b[0m     }\n\u001b[0;32m    908\u001b[0m )\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3073\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3462\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3460\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_row(example\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3462\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3463\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_writer.py:537\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[1;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 537\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_writer.py:495\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    492\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[0;32m    493\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[0;32m    494\u001b[0m         ]\n\u001b[1;32m--> 495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_writer.py:605\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    603\u001b[0m         col_try_type \u001b[38;5;241m=\u001b[39m try_features[col] \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    604\u001b[0m         typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 605\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    606\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    607\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\pyarrow\\array.pxi:250\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\pyarrow\\array.pxi:114\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\arrow_writer.py:228\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     out \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39marray(\u001b[43mcast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_int_optimization:\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\features\\features.py:456\u001b[0m, in \u001b[0;36mcast_to_python_objects\u001b[1;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcast_to_python_objects\u001b[39m(obj: Any, only_1d_for_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, optimize_list_casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    It works recursively.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m        casted_obj: the casted object\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize_list_casting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_list_casting\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\features\\features.py:418\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[1;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[0;32m    412\u001b[0m casted_first_elmt, has_changed_first_elmt \u001b[38;5;241m=\u001b[39m _cast_to_python_objects(\n\u001b[0;32m    413\u001b[0m     first_elmt, only_1d_for_numpy\u001b[38;5;241m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[38;5;241m=\u001b[39moptimize_list_casting\n\u001b[0;32m    414\u001b[0m )\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_changed_first_elmt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optimize_list_casting:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    417\u001b[0m         [\n\u001b[1;32m--> 418\u001b[0m             \u001b[43m_cast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m                \u001b[49m\u001b[43melmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize_list_casting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_list_casting\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    421\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m elmt \u001b[38;5;129;01min\u001b[39;00m obj\n\u001b[0;32m    422\u001b[0m         ],\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m     )\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\features\\features.py:368\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[1;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    359\u001b[0m             [\n\u001b[0;32m    360\u001b[0m                 _cast_to_python_objects(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    366\u001b[0m         )\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mPIL_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPIL\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pd\u001b[38;5;241m.\u001b[39mSeries):\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    371\u001b[0m         _cast_to_python_objects(\n\u001b[0;32m    372\u001b[0m             obj\u001b[38;5;241m.\u001b[39mtolist(), only_1d_for_numpy\u001b[38;5;241m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[38;5;241m=\u001b[39moptimize_list_casting\n\u001b[0;32m    373\u001b[0m         )[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    374\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    375\u001b[0m     )\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\features\\image.py:315\u001b[0m, in \u001b[0;36mencode_pil_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: image\u001b[38;5;241m.\u001b[39mfilename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mimage_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m}\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\datasets\\features\\image.py:307\u001b[0m, in \u001b[0;36mimage_to_bytes\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPNG\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIFF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 307\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\PIL\\Image.py:2439\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2436\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2439\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1402\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1398\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1399\u001b[0m         im, fp, chunk, rawmode, default_image, append_images\n\u001b[0;32m   1400\u001b[0m     )\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\PIL\\ImageFile.py:540\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    538\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 540\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    542\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32md:\\Documents\\MASC\\NLP_EMO\\.venv312\\Lib\\site-packages\\PIL\\ImageFile.py:559\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    560\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"cairocode/MSP_Pod_SYL6\")\n",
    "# Assuming your Hugging Face dataset is named `updated_dataset2`\n",
    "dataset = updated_dataset2\n",
    "\n",
    "# Step 1: Get unique `wav file_name` values\n",
    "unique_file_names = list(set(dataset['wav_filename']))\n",
    "\n",
    "# Step 2: Split the unique file names into train, validation, and test sets\n",
    "train_files, temp_files = train_test_split(unique_file_names, test_size=0.4, random_state=42)  # 60% train\n",
    "val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)           # 20% val, 20% test\n",
    "\n",
    "# Step 3: Create a function to assign splits\n",
    "def assign_split(example):\n",
    "    if example['wav_filename'] in train_files:\n",
    "        return 'train'\n",
    "    elif example['wav_filename'] in val_files:\n",
    "        return 'validation'\n",
    "    else:\n",
    "        return 'test'\n",
    "\n",
    "# Step 4: Add the split column to the dataset\n",
    "dataset = dataset.map(lambda example: {'split': assign_split(example)})\n",
    "\n",
    "# Step 5: Split the dataset into train, validation, and test subsets\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset.filter(lambda example: example['split'] == 'train'),\n",
    "    'validation': dataset.filter(lambda example: example['split'] == 'validation'),\n",
    "    'test': dataset.filter(lambda example: example['split'] == 'test')\n",
    "})\n",
    "\n",
    "# Remove the 'split' column if no longer needed\n",
    "dataset_dict = dataset_dict.map(lambda example: {k: v for k, v in example.items() if k != 'split'})\n",
    "\n",
    "# # Verify the splits\n",
    "# print(dataset_dict)\n",
    "dataset_dict.push_to_hub(\"MSPP_SYL_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500525/500525 [05:26<00:00, 1533.06 examples/s]\n",
      "Filter: 100%|██████████| 500525/500525 [05:02<00:00, 1654.34 examples/s]\n",
      "Filter: 100%|██████████| 500525/500525 [04:56<00:00, 1685.63 examples/s]\n",
      "Filter: 100%|██████████| 500525/500525 [04:55<00:00, 1695.07 examples/s]\n",
      "Map: 100%|██████████| 42905/42905 [00:05<00:00, 8307.40 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 230.61ba/s]\n",
      "Map: 100%|██████████| 42905/42905 [00:05<00:00, 8443.72 examples/s]1.72s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 243.35ba/s]\n",
      "Map: 100%|██████████| 42905/42905 [00:05<00:00, 8418.81 examples/s]0.41s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 251.11ba/s]\n",
      "Map: 100%|██████████| 42905/42905 [00:05<00:00, 8494.22 examples/s]1.78s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 242.65ba/s]\n",
      "Map: 100%|██████████| 42905/42905 [00:05<00:00, 8258.33 examples/s]2.34s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 252.84ba/s]\n",
      "Map: 100%|██████████| 42905/42905 [00:05<00:00, 8408.10 examples/s]0.24s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 217.03ba/s]\n",
      "Map: 100%|██████████| 42904/42904 [00:05<00:00, 8246.22 examples/s]2.77s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 430/430 [00:01<00:00, 245.79ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 7/7 [17:53<00:00, 153.41s/it]\n",
      "Map: 100%|██████████| 33328/33328 [00:03<00:00, 8386.45 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 334/334 [00:01<00:00, 250.47ba/s]\n",
      "Map: 100%|██████████| 33328/33328 [00:03<00:00, 8384.20 examples/s]8.49s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 334/334 [00:01<00:00, 247.15ba/s]\n",
      "Map: 100%|██████████| 33328/33328 [00:04<00:00, 8077.58 examples/s]2.67s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 334/334 [00:01<00:00, 235.14ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [05:38<00:00, 112.84s/it]\n",
      "Map: 100%|██████████| 33403/33403 [00:04<00:00, 8266.37 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 335/335 [00:01<00:00, 239.73ba/s]\n",
      "Map: 100%|██████████| 33402/33402 [00:03<00:00, 8454.34 examples/s]2.48s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 335/335 [00:01<00:00, 251.34ba/s]\n",
      "Map: 100%|██████████| 33402/33402 [00:04<00:00, 8203.28 examples/s]2.27s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 335/335 [00:01<00:00, 240.60ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [05:39<00:00, 113.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cairocode/MSPP_SYL_V2/commit/a1804e912dc187b57fa416544729ca3314abc907', commit_message='Upload dataset', commit_description='', oid='a1804e912dc187b57fa416544729ca3314abc907', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/cairocode/MSPP_SYL_V2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='cairocode/MSPP_SYL_V2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"cairocode/MSPP_SYL_FULL\")\n",
    "# Assuming your Hugging Face dataset is named `updated_dataset2`\n",
    "dataset = ds['train']\n",
    "\n",
    "# Step 1: Get unique `wav file_name` values\n",
    "unique_file_names = list(set(dataset['wav_filename']))\n",
    "\n",
    "# Step 2: Split the unique file names into train, validation, and test sets\n",
    "train_files, temp_files = train_test_split(unique_file_names, test_size=0.4, random_state=42)  # 60% train\n",
    "val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)           # 20% val, 20% test\n",
    "\n",
    "# Step 3: Create a function to assign splits\n",
    "def assign_split(example):\n",
    "    if example['wav_filename'] in train_files:\n",
    "        return 'train'\n",
    "    elif example['wav_filename'] in val_files:\n",
    "        return 'validation'\n",
    "    else:\n",
    "        return 'test'\n",
    "\n",
    "# Step 4: Add the split column to the dataset\n",
    "dataset = dataset.map(lambda example: {'split': assign_split(example)})\n",
    "\n",
    "# Step 5: Split the dataset into train, validation, and test subsets\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset.filter(lambda example: example['split'] == 'train'),\n",
    "    'validation': dataset.filter(lambda example: example['split'] == 'validation'),\n",
    "    'test': dataset.filter(lambda example: example['split'] == 'test')\n",
    "})\n",
    "\n",
    "# # Remove the 'split' column if no longer needed\n",
    "# dataset_dict = dataset_dict.map(lambda example: {k: v for k, v in example.items() if k != 'split'})\n",
    "\n",
    "# # Verify the splits\n",
    "# print(dataset_dict)\n",
    "dataset_dict.push_to_hub(\"MSPP_SYL_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label', 'speaker', 'full_path', 'arousal', 'valence', 'wav_filename', 'syl_number', 'EmoClass', 'EmoAct', 'EmoVal', 'EmoDom', 'SpkrID', 'Gender', 'Split_Set', 'transcript'],\n",
      "    num_rows: 500525\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(updated_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 41711/41711 [00:02<00:00, 17457.57 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 220.30ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:02<00:00, 15252.48 examples/s]0.47s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:02<00:00, 171.22ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:02<00:00, 16320.61 examples/s]6.61s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:02<00:00, 183.10ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:01<00:00, 21977.98 examples/s]0.39s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 230.62ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:01<00:00, 31802.82 examples/s]1.20s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 253.54ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:01<00:00, 30953.27 examples/s]2.37s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 244.56ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:01<00:00, 30696.01 examples/s]3.22s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 262.92ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:01<00:00, 32141.31 examples/s]2.10s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 264.98ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 17338.28 examples/s]1.76s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 234.23ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 17810.91 examples/s]4.15s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 217.54ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 14410.67 examples/s]80.76s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 214.26ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 18285.26 examples/s]74.48s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 212.50ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 12/12 [38:11<00:00, 190.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cairocode/MSPP_SYL_FULL/commit/b1adc3eb44b29d60076a5f284d23c982036464a2', commit_message='Upload dataset', commit_description='', oid='b1adc3eb44b29d60076a5f284d23c982036464a2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/cairocode/MSPP_SYL_FULL', endpoint='https://huggingface.co', repo_type='dataset', repo_id='cairocode/MSPP_SYL_FULL'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset2.push_to_hub(\"MSPP_SYL_FULL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
