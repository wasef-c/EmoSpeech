{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      "1. CMU MOSEI\n",
      "2. CREMA-D\n",
      "3. EMOV-DB\n",
      "4. MSP IMPROV\n",
      "5. RAVDESS-DB\n",
      "6. TESS-DB\n",
      "7. VIVAE-DB\n",
      "8. IEMOCAP\n",
      "9. ASVP-ESD\n",
      "10. OMG\n",
      "11. MSP_POD\n",
      "12. MSP_POD_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import parselmouth\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Define constants\n",
    "DATASETS_PATH = r'D:\\Documents\\MASC\\MSP_POD_dataset'\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.io import wavfile\n",
    "from PIL import Image\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "def clean_audio(path):\n",
    "    y, sr = librosa.load(path)\n",
    "    S_full, phase = librosa.magphase(librosa.stft(y))\n",
    "    idx = slice(*librosa.time_to_frames([2, 6], sr=sr))\n",
    "    width = int((S_full.shape[-1] - 1)/2)-1\n",
    "    S_filter = librosa.decompose.nn_filter(S_full,\n",
    "                                           aggregate=np.median,\n",
    "                                           metric='cosine',\n",
    "                                           width=width)\n",
    "    S_filter = np.minimum(S_full, S_filter)\n",
    "    margin_i, margin_v = 2, 10\n",
    "    power = 2\n",
    "\n",
    "    mask_i = librosa.util.softmask(S_filter,\n",
    "                                   margin_i * (S_full - S_filter),\n",
    "                                   power=power)\n",
    "\n",
    "    mask_v = librosa.util.softmask(S_full - S_filter,\n",
    "                                   margin_v * S_filter,\n",
    "                                   power=power)\n",
    "\n",
    "    S_foreground = mask_v * S_full\n",
    "\n",
    "    sound = librosa.istft(S_foreground * phase)\n",
    "    # sound = y\n",
    "    # # # sf.write(os.path.join(new_dir,new_name), librosa.istft(S_foreground * phase), sr)\n",
    "    return sound, sr\n",
    "# Step 1: Create a Mel-Spectrogram\n",
    "def create_mel_spectrogram(audio, sr):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    return mel_spec\n",
    "\n",
    "# Step 2: Extract syllables (this is a simplified approach)\n",
    "def extract_syllables(mel_spec):\n",
    "    energy = np.sum(mel_spec, axis=0)\n",
    "    peaks, _ = find_peaks(energy, distance=20)  # Adjust distance as needed\n",
    "    if len(peaks) < 2:\n",
    "        # If less than 2 peaks found, use start and end of the spectrogram\n",
    "        peaks = np.array([0, mel_spec.shape[1] - 1])\n",
    "    return peaks\n",
    "\n",
    "# Step 3: Extract formants for a specific time range\n",
    "def extract_formants_for_syllable(audio, sr, start_time, end_time):\n",
    "    sound = parselmouth.Sound(audio, sampling_frequency=sr)\n",
    "    syllable_sound = sound.extract_part(from_time=start_time, to_time=end_time)\n",
    "    formant = syllable_sound.to_formant_burg()\n",
    "    return formant\n",
    "\n",
    "# Step 4: Plot formants and spectrogram for each syllable\n",
    "def plot_syllable(fname, image_dir,mel_spec, sr, syl_val, syllable_start, syllable_end, formant):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(2.24, 2.24))\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    librosa.display.specshow(librosa.power_to_db(mel_spec[:, syllable_start:syllable_end], ref=np.max), ax=ax1)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # Plot formants\n",
    "    times = formant.xs()\n",
    "    formant_values = []\n",
    "    for t in times:\n",
    "        for i in range(1, 4):\n",
    "            value = formant.get_value_at_time(formant_number=i, time=t)\n",
    "            if value is not None:\n",
    "                formant_values.append(value)\n",
    "    \n",
    "    # Ensure times and formant_values have the same length\n",
    "    times_repeated = np.repeat(times, 3)[:len(formant_values)]\n",
    "    \n",
    "    ax2.scatter(times_repeated, formant_values, s=1)\n",
    "    ax2.set_ylim(0, 5000)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    jpname = f\"{fname}_syl{syl_val}.png\"\n",
    "    save_path = os.path.join(image_dir, jpname)\n",
    "\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    return jpname\n",
    "\n",
    "# Main function to process the audio\n",
    "def process_audio(audio_path, fname, image_dir):\n",
    "    jpnames = []\n",
    "    # Clean the audio\n",
    "    audio, sr = clean_audio(audio_path)\n",
    "    \n",
    "    # Create mel spectrogram\n",
    "    mel_spec = create_mel_spectrogram(audio, sr)\n",
    "    \n",
    "    # Extract syllables\n",
    "    syllable_boundaries = extract_syllables(mel_spec)\n",
    "    \n",
    "    # Process each syllable\n",
    "\n",
    "    for i in range(len(syllable_boundaries) - 1):\n",
    "        start_frame = syllable_boundaries[i]\n",
    "        end_frame = syllable_boundaries[i+1]\n",
    "        \n",
    "        # Convert frames to time\n",
    "        start_time = librosa.frames_to_time(start_frame, sr=sr)\n",
    "        end_time = librosa.frames_to_time(end_frame, sr=sr)\n",
    "        \n",
    "        # Extract formants for the syllable\n",
    "        formant = extract_formants_for_syllable(audio, sr, start_time, end_time)\n",
    "        \n",
    "        # Plot the syllable\n",
    "        jpname= plot_syllable(fname,image_dir, mel_spec, sr, i,start_frame, end_frame, formant)\n",
    "        jpnames.append(jpname)\n",
    "    return  jpnames \n",
    "\n",
    "\n",
    "def process_files(image_dir, csv_dir, ds=0):\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    csv_file = os.path.join(csv_dir, 'file_labels.csv')\n",
    "\n",
    "    existing_files = set()\n",
    "    if os.path.exists(csv_file):\n",
    "        with open(csv_file, 'r', newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                existing_files.add(row['file_name'])\n",
    "\n",
    "\n",
    "\n",
    "    for filename in os.listdir(csv_dir):\n",
    "        if filename.endswith('.wav'):  # Check for .wav extension\n",
    "            file_path = os.path.join(csv_dir, filename)\n",
    "            if os.path.isfile(file_path):  # Ensure it's a file\n",
    "                fname = os.path.basename(file_path)\n",
    "                jpnames =  process_audio(file_path, fname, image_dir)\n",
    "\n",
    "\n",
    "    # with open(csv_file, 'a', newline='') as csvfile:\n",
    "    #     fieldnames = ['file_name', 'full_path', 'speaker', 'full_path','arousal', 'valence', '']\n",
    "    #     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    #     if os.path.getsize(csv_file) == 0:\n",
    "    #         writer.writeheader()\n",
    "\n",
    "    #     total_new_images = 0\n",
    "    #     for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing audio files\"):\n",
    "    #         file_path = row['filename']\n",
    "    #         file_name = row['fname']\n",
    "    #         label_val = row['label']\n",
    "    #         speaker = row['speaker']\n",
    "    #         arousal = row['act']\n",
    "    #         valence = row['val']\n",
    "    #         domination = row['']\n",
    "            \n",
    "    #         if ds == 0:\n",
    "    #             fname = str(file_name.split('.wav')[0])\n",
    "    #         else:\n",
    "    #             indval = file_path.rfind('/', 0, file_path.rfind('/'))\n",
    "    #             vid_utter = file_path[indval+1:]\n",
    "    #             fname = vid_utter.replace(\"/\", \"_\")\n",
    "            \n",
    "    #         jpnames =  process_audio(file_path, fname, image_dir)\n",
    "\n",
    "    #         for jpname in jpnames:\n",
    "    #             full__path = os.path.join(image_dir, jpname)\n",
    "    #             writer.writerow({\n",
    "    #                 'file_name': jpname,\n",
    "    #                 'label': label_val,\n",
    "    #                 'speaker': speaker,\n",
    "    #                 'full_path': full__path,\n",
    "    #                 'arousal': arousal,\n",
    "    #                 'valence': valence,\n",
    "    #                 'domination' : domination,\n",
    "    #             })\n",
    "    \n",
    "\n",
    "def main():\n",
    "    datasets = {\n",
    "        1: (\"CMU MOSEI\", '/home/carol/Documents/Emo_rec/CSV_FILES/CMUmini_data.csv', '/media/carol/Data/DATASETS/SavedSets002/CMUmini'),\n",
    "        2: (\"CREMA-D\", '/home/carol/Documents/Emo_rec/CSV_FILES/CREMA_data.csv', '/media/carol/Data/DATASETS/SavedSets002/CREMA'),\n",
    "        3: (\"EMOV-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/EMOV_data.csv', '/media/carol/Data/DATASETS/SavedSets002/EMOV'),\n",
    "        4: (\"MSP IMPROV\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/MSPIMPROV_data2.csv', '/media/carol/Data/DATASETS/SavedSets002/MSPI_SYL'),\n",
    "        5: (\"RAVDESS-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/RAVDESS_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/RAVDESS'),\n",
    "        6: (\"TESS-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/TESS_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/TESS'),\n",
    "        7: (\"VIVAE-DB\", '/home/carol/Documents/Emo_rec/CSV_FILES/VIVAE_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/VIVAE'),\n",
    "        8: (\"IEMOCAP\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/IEMOCAP_data_Full.csv', '/media/carol/Data/DATASETS/SavedSets002/IEMOCAP_SYL'),\n",
    "        9: (\"ASVP-ESD\", '/home/carol/Documents/Emo_rec/CSV_FILES/ASVP_data.csv', '/media/carol/Data/DATASETS/SavedSets002/Archive/ASVP'),\n",
    "        10: (\"OMG\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/OMG_Train.csv', '/media/carol/Data/DATASETS/SavedSets002/OMG_Train2'),\n",
    "        11: (\"MSP_POD\", '/media/carol/Data/Documents/Emo_rec/CSV_FILES/MSPpod_data.csv', '/media/carol/Data/DATASETS/SavedSets002/MSP_POD_MEL'),\n",
    "        12:(\"MSP_POD_2\", r'D:\\Documents\\MASC\\MSP_POD_dataset\\Audios\\Audios.tar\\test', r'D:\\Documents\\MASC\\MSP_POD_dataset\\Images_Syllables_TEST')\n",
    "    }\n",
    "\n",
    "    print(\"Available datasets:\")\n",
    "    for key, (name, _, _) in datasets.items():\n",
    "        print(f\"{key}. {name}\")\n",
    "\n",
    "    dataset_choice = int(input(\"Enter the number of the dataset you would like to run: \"))\n",
    "\n",
    "    if dataset_choice in datasets:\n",
    "        name, file_path, output_dir = datasets[dataset_choice]\n",
    "        # print(f\"--------------{name} DATASET STARTED ---- \")\n",
    "\n",
    "        # df = pd.read_csv(file_path)\n",
    "        # image_dir = os.path.join(output_dir, 'images')\n",
    "        # os.makedirs(image_dir, exist_ok=True)\n",
    "        # os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        process_files(output_dir, file_path, dataset_choice)\n",
    "    else:\n",
    "        print(\"Invalid dataset choice.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created successfully and saved to D:\\Documents\\MASC\\MSP_POD_dataset\\Images_Syllables_TEST\\metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "image_folder = r\"D:\\Documents\\MASC\\MSP_POD_dataset\\Images_Syllables_TEST\"  # Replace with the path to your folder of .png files\n",
    "existing_csv_path = r\"D:\\Documents\\MASC\\MSP_POD_dataset\\Audios\\Audios.tar\\test\\metadata.csv\"  # Replace with the path to your existing CSV\n",
    "output_csv_path = os.path.join(image_folder, \"metadata.csv\") # Replace with the path to save the final CSV\n",
    "\n",
    "# Load the existing CSV\n",
    "existing_csv = pd.read_csv(existing_csv_path)\n",
    "\n",
    "# Create a mapping from wav_filename to transcript\n",
    "wav_to_transcript = dict(zip(existing_csv['file_name'], existing_csv['transcript']))\n",
    "\n",
    "# List all .png files in the folder\n",
    "png_files = [f for f in os.listdir(image_folder) if f.endswith('.png')]\n",
    "\n",
    "# Prepare the final data\n",
    "data = []\n",
    "for png_file in png_files:\n",
    "    # Extract the .wav file name from the .png file name\n",
    "    wav_filename = png_file.split('.')[0] + '.wav'\n",
    "    extracted_number = png_file.split('_')[-1].split('.')[0]\n",
    "    # Get the mapped transcript\n",
    "    transcript = wav_to_transcript.get(wav_filename, \"Transcript not found\")\n",
    "    \n",
    "    # Append to the data list\n",
    "    data.append({\n",
    "        \"file_name\": png_file,\n",
    "        \"file\": wav_filename,\n",
    "        \"syllable\": extracted_number,\n",
    "        \"Transcript\": transcript\n",
    "    })\n",
    "\n",
    "# Create a DataFrame and save it as CSV\n",
    "output_df = pd.DataFrame(data)\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"CSV created successfully and saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 26249/26249 [00:01<00:00, 14893.98files/s]\n",
      "Generating test split: 26248 examples [00:02, 9892.92 examples/s] \n",
      "Map: 100%|██████████| 26248/26248 [01:56<00:00, 225.85 examples/s]]\n",
      "Creating parquet from Arrow format: 100%|██████████| 263/263 [00:00<00:00, 310.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [03:53<00:00, 233.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cairocode/MSPP_TEST_SYL/commit/0d038efa9bf2ae8fd96b2b2db5af342ef2e0f8dd', commit_message='Upload dataset', commit_description='', oid='0d038efa9bf2ae8fd96b2b2db5af342ef2e0f8dd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/cairocode/MSPP_TEST_SYL', endpoint='https://huggingface.co', repo_type='dataset', repo_id='cairocode/MSPP_TEST_SYL'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the image folder\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=image_folder)\n",
    "dataset.push_to_hub(\"MSPP_TEST_SYL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 333683/333683 [00:03<00:00, 88749.44 examples/s] \n",
      "Generating validation split: 100%|██████████| 83421/83421 [00:00<00:00, 90657.58 examples/s]\n",
      "Generating test split: 100%|██████████| 83421/83421 [00:00<00:00, 93817.50 examples/s] \n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"cairocode/MSP_Pod_SYL6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets([ds['train'], ds['test'], ds['validation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500525/500525 [00:37<00:00, 13463.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_wav_and_syl(example):\n",
    "    # Extract wav_filename using regex\n",
    "    wav_match = re.search(r'(?<=Audios_)(.*?\\.wav)', example['full_path'])\n",
    "    syl_match = re.search(r'_(\\d+)\\.wav', example['full_path'])\n",
    "    \n",
    "    # Assign extracted values or None if no match\n",
    "    example['wav_filename'] = wav_match.group(1) if wav_match else None\n",
    "    example['syl_number'] = syl_match.group(1) if syl_match else None\n",
    "    return example\n",
    "\n",
    "# Map the function to the dataset\n",
    "updated_dataset = combined_dataset.map(extract_wav_and_syl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=224x223>,\n",
       " 'label': 1,\n",
       " 'speaker': 1,\n",
       " 'full_path': 'C:\\\\Users\\\\Paolo\\\\Documents\\\\carol_emo_rec\\\\DATASETS\\\\Image_Sets\\\\MSP_POD_SYL\\\\images\\\\Audios_MSP-PODCAST_0003_0360.wav_syl1.png',\n",
       " 'arousal': 6.076923,\n",
       " 'valence': 5.846154,\n",
       " 'wav_filename': 'MSP-PODCAST_0003_0360.wav',\n",
       " 'syl_number': '0360'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   file_name EmoClass  EmoAct  EmoVal  EmoDom  SpkrID  Gender  \\\n",
      "0  MSP-PODCAST_0002_0033.wav        N     4.8     4.2     5.4     127  Female   \n",
      "1  MSP-PODCAST_0002_0039.wav        N     4.0     4.2     4.2     127  Female   \n",
      "2  MSP-PODCAST_0002_0051.wav        N     4.0     4.2     4.2     127  Female   \n",
      "3  MSP-PODCAST_0002_0059.wav        X     4.0     3.8     4.0     128  Female   \n",
      "4  MSP-PODCAST_0002_0061.wav        F     3.4     2.8     4.2     128  Female   \n",
      "\n",
      "     Split_Set                                         transcript  \n",
      "0  Development               and i mean the numbers. right? so...  \n",
      "1  Development  15 hundred or something. it talks about the tw...  \n",
      "2  Development  [inaudible 00:03:13] so here it is. so it's pa...  \n",
      "3  Development  no. it sounds like it could be like a bourne i...  \n",
      "4  Development  yeah. so, but molly, i mean, this was really o...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_path = r\"D:\\Documents\\MASC\\MSP_POD_dataset\\Audios\\Audios.tar\\Audios\\metadata.csv\"\n",
    "\n",
    "# Load CSV into a DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500525/500525 [43:57<00:00, 189.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label', 'speaker', 'full_path', 'arousal', 'valence', 'wav_filename', 'syl_number'],\n",
      "    num_rows: 500525\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_csv_info(example):\n",
    "    matching_row = df[df['file_name'] == example['wav_filename']]\n",
    "    if not matching_row.empty:\n",
    "        for col in df.columns:\n",
    "            if col != 'file_name':  # Skip the file_name column\n",
    "                value = matching_row.iloc[0][col]\n",
    "                if pd.isnull(value):  # Handle missing values\n",
    "                    example[col] = None\n",
    "                elif isinstance(value, (int, float)):  # Keep numbers as is\n",
    "                    example[col] = value\n",
    "                else:  # Convert everything else to string\n",
    "                    example[col] = str(value)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            if col != 'file_name':\n",
    "                example[col] = None  # Default value for no match\n",
    "    return example\n",
    "\n",
    "\n",
    "# Map the function to the dataset\n",
    "updated_dataset2 = updated_dataset.map(add_csv_info)\n",
    "\n",
    "# Inspect the updated dataset\n",
    "print(updated_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label', 'speaker', 'full_path', 'arousal', 'valence', 'wav_filename', 'syl_number', 'EmoClass', 'EmoAct', 'EmoVal', 'EmoDom', 'SpkrID', 'Gender', 'Split_Set', 'transcript'],\n",
      "    num_rows: 500525\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(updated_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 41711/41711 [00:02<00:00, 17457.57 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 220.30ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:02<00:00, 15252.48 examples/s]0.47s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:02<00:00, 171.22ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:02<00:00, 16320.61 examples/s]6.61s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:02<00:00, 183.10ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:01<00:00, 21977.98 examples/s]0.39s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 230.62ba/s]\n",
      "Map: 100%|██████████| 41711/41711 [00:01<00:00, 31802.82 examples/s]1.20s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 253.54ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:01<00:00, 30953.27 examples/s]2.37s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 244.56ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:01<00:00, 30696.01 examples/s]3.22s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 262.92ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:01<00:00, 32141.31 examples/s]2.10s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 264.98ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 17338.28 examples/s]1.76s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 234.23ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 17810.91 examples/s]4.15s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 217.54ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 14410.67 examples/s]80.76s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 214.26ba/s]\n",
      "Map: 100%|██████████| 41710/41710 [00:02<00:00, 18285.26 examples/s]74.48s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 418/418 [00:01<00:00, 212.50ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 12/12 [38:11<00:00, 190.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cairocode/MSPP_SYL_FULL/commit/b1adc3eb44b29d60076a5f284d23c982036464a2', commit_message='Upload dataset', commit_description='', oid='b1adc3eb44b29d60076a5f284d23c982036464a2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/cairocode/MSPP_SYL_FULL', endpoint='https://huggingface.co', repo_type='dataset', repo_id='cairocode/MSPP_SYL_FULL'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_dataset2.push_to_hub(\"MSPP_SYL_FULL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
